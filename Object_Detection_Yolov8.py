# -*- coding: utf-8 -*-
"""Team7_IOSB_Task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CQmvPwJFzHFEEIPmHKhkPG3mzuODn8TG
"""

from google.colab import drive

drive.mount("/content/gdrive")

!git clone https://github.com/ultralytics/ultralytics.git

pip install pyrealsense2

import pyrealsense2 as rs
import numpy as np
import cv2
from google.colab.patches import cv2_imshow

# Commented out IPython magic to ensure Python compatibility.
# %pip install ultralytics
import ultralytics
ultralytics.checks()

# Download COCO val
import torch
torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)
!unzip -q tmp.zip -d datasets && rm tmp.zip  # unzip

# Validate YOLOv8n on COCO8 val
!yolo val model=yolov8n.pt data=coco8.yaml

# Load YOLOv8n-seg, train it on COCO128-seg for 3 epochs and predict an image with it
from ultralytics import YOLO

model = YOLO('yolov8n-seg.pt')

# Configure depth and color streams
pipeline = rs.pipeline()
config = rs.config()
config.enable_device_from_file("/content/gdrive/MyDrive/Copy of 20240525_201536.bag", repeat_playback=False)

import os
# Create a directory to store frames
output_dir = 'frames'
os.makedirs(output_dir, exist_ok=True)
# Directory containing the stored frames
input_dir = 'frames'

"""Streams depth and color frames from a file using the given pipeline and configuration,
    processes them to extract numpy arrays, and saves each frame as an image file in the specified output directory.

    Args:
    - pipeline (pyrealsense2.pipeline): The RealSense pipeline for streaming frames.
    - config (pyrealsense2.config): The configuration for the RealSense pipeline.
    - output_dir (str): The directory where the captured frames will be saved.

    Workflow:
    1. Create the output directory if it doesn't exist.
    2. Start the streaming pipeline with the given configuration.
    3. Continuously capture frames until the end of the stream:
       a. Wait for a coherent pair of frames (depth and color).
       b. Convert the frames to numpy arrays.
       c. Save the depth and color frames as PNG images in the output directory with sequential filenames.
    4. Handle any runtime errors that occur when reading frames.
    5. Stop the streaming pipeline when done.

"""

# Start streaming from the file
def stream_and_save_frames(pipeline, config, output_dir):

  os.makedirs(output_dir, exist_ok=True)
  pipeline.start(config)

  try:
    frame_count = 0
    while True:
        # Wait for a coherent pair of frames: depth and color
        frames = pipeline.wait_for_frames()
        depth_frame = frames.get_depth_frame()
        color_frame = frames.get_color_frame()
        if not depth_frame or not color_frame:
            continue

        # Convert images to numpy arrays
        depth_image = np.asanyarray(depth_frame.get_data())
        color_image = np.asanyarray(color_frame.get_data())

        # Save images
        depth_filename = os.path.join(output_dir, f'depth_frame_{frame_count:06d}.png')
        color_filename = os.path.join(output_dir, f'color_frame_{frame_count:06d}.png')
        cv2.imwrite(depth_filename, depth_image)
        cv2.imwrite(color_filename, color_image)

        frame_count += 1

  except RuntimeError as e:
    print("Finished reading all frames from the bag file.")

  finally:
    # Stop streaming
    pipeline.stop()

# Load the class names from coco.yaml
import yaml
with open("/content/ultralytics/ultralytics/cfg/datasets/coco.yaml", "r") as f:
    coco_data = yaml.safe_load(f)
    classes = coco_data['names']

# Prepare VideoWriter
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter('segmented_output.avi', fourcc, 30.0, (640, 480))

"""Processes the input color and depth images by performing object segmentation using a pre-trained YOLOv8 model,
    applying random color shading to each segmented object, calculating the average depth of each segmented area,
    and drawing contours and labels on the color image. The processed image is then written to a video file.

    Args:
    - color_image (numpy.ndarray): The input color image in which objects will be segmented and processed.
    - depth_image (numpy.ndarray): The corresponding depth image used to calculate the average depth of segmented objects.

    Workflow:
    1. Apply the YOLOv8 model to the color image to obtain segmentation results.
    2. Filter the segmentation results based on confidence threshold (e.g., 0.5).
    3. For each filtered segmentation:
       a. Extract and binarize the mask.
       b. Apply random color shading to the segmented area.
       c. Calculate the average depth for the segmented area using the depth image.
       d. Find and draw contours around the segmented area on the color image.
       e. Label each segmented area with its class name and average depth.
    4. Optionally display the processed color and depth images (commented out).
    5. Write the processed color image frame into the video file.

"""

def process_images(color_image, depth_image):

    # YOLOv8 segmentation
    results = model(color_image)

    # Draw segmentation results on the image
    for result in results:
      # Filter results based on confidence
        filtered_boxes = [i for i, conf in enumerate(result.boxes.conf) if conf > 0.5]
        if result.masks:
            for seg_idx in filtered_boxes:
                mask = result.masks.data[seg_idx].cpu().numpy()
                mask = (mask > 0.5).astype(np.uint8)  # Binarize the mask
                color_shade = np.random.randint(0, 255, size=3)  # Random color for shading

                # Shade the segment
                color_image[mask == 1] = color_image[mask == 1] * 0.5 + color_shade * 0.5

                # Calculate the average depth for the segment
                avg_depth = np.mean(depth_image[mask == 1])

                # Find contours to draw and label
                contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
                cv2.drawContours(color_image, contours, -1, (0, 255, 0), 2)

                # Get the class label
                class_id = int(result.boxes.cls[seg_idx])
                label = f'{classes[class_id]}: {avg_depth:.2f}m'

                for cnt in contours:
                    x, y, w, h = cv2.boundingRect(cnt)
                    cv2.putText(color_image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

    # Show images
    #cv2_imshow(color_image)
    #cv2_imshow(depth_image)

    # Write the frame into the video file
    out.write(color_image)

#Read the images file path
depth_files = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.startswith('depth_frame_') and f.endswith('.png')])
color_files = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.startswith('color_frame_') and f.endswith('.png')])

"""Processes pairs of RGB and depth images, performs segmentation on each pair,
    and writes the segmented color images into an output video file.

    Args:
    - color_files (list of str): List of file paths to the color images.
    - depth_files (list of str): List of file paths to the depth images.
    - output_video_path (str): Path to the output video file.
    - frame_size (tuple): Size of each frame in the output video (default is (640, 480)).
    - fps (int): Frames per second for the output video (default is 30).

    Workflow:
    1. Initialize the video writer with the specified parameters.
    2. Loop through each pair of color and depth files:
       a. Load the color image.
       b. Load the depth image (as an unchanged format).
       c. Normalize the depth image to the range [0, 1].
       d. Call the `process_images` function to segment the color image and overlay depth information.
       e. Write the processed color image into the video file.
    3. Release the video writer after processing all images.
"""

def prepare_and_process():
  for color_file,depth_file in zip(color_files,depth_files):
    color_image = cv2.imread(color_file)  # Load color image
    depth_image = cv2.imread(depth_file, cv2.IMREAD_UNCHANGED)  # Load depth image

    # Normalize depth image to range [0, 1]
    depth_image = depth_image.astype(np.float32)
    depth_image = cv2.normalize(depth_image, None, 0, 1, cv2.NORM_MINMAX)
    process_images(color_image, depth_image)

  # Release the video writer
  out.release()

stream_and_save_frames(pipeline, config, output_dir)

prepare_and_process()